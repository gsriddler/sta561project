{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all required strings\n",
    "from PreProcessor import PreProcessor\n",
    "from Encoder import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreProcessor.__init()__: Data Imported\n"
     ]
    }
   ],
   "source": [
    "train_file_name = \"train.tsv\"\n",
    "labels = [\n",
    "        'id',                # Column 1: the ID of the statement ([ID].json).\n",
    "        'label',             # Column 2: the label.\n",
    "        'statement',         # Column 3: the statement.\n",
    "        'subjects',          # Column 4: the subject(s).\n",
    "        'speaker',           # Column 5: the speaker.\n",
    "        'speaker_job_title', # Column 6: the speaker's job title.\n",
    "        'state_info',        # Column 7: the state info.\n",
    "        'party_affiliation', # Column 8: the party affiliation.\n",
    "        \n",
    "        # Column 9-13: the total credit history count, including the current statement.\n",
    "        'count_1', # barely true counts.\n",
    "        'count_2', # false counts.\n",
    "        'count_3', # half true counts.\n",
    "        'count_4', # mostly true counts.\n",
    "        'count_5', # pants on fire counts.\n",
    "        \n",
    "        'context' # Column 14: the context (venue / location of the speech or statement).\n",
    "    ]\n",
    "\n",
    "#initialize the PreProcessor\n",
    "pre_processor = PreProcessor(verbose=True)\n",
    "pre_processor.import_data_from_file(\n",
    "    file_name=\"train.tsv\",\n",
    "    deliminator='\\t',\n",
    "    headers = labels,\n",
    "    replace_Null_NaN=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding_mappings: {'pants-fire': 0, 'false': 1, 'barely-true': 2, 'half-true': 3, 'mostly-true': 4, 'true': 5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#set the label column\n",
    "label_mapping = {'pants-fire':0,\n",
    "             'false':1,\n",
    "             'barely-true':2,\n",
    "             'half-true':3,\n",
    "             'mostly-true':4,\n",
    "             'true':5}\n",
    "pre_processor.set_label_header(\"label\",label_mapping,normalize_encoding=False)\n",
    "\n",
    "df = pre_processor.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interview', 'speech', 'ad', 'news', 'campaign', 'debate', 'press', 'release', 'state', 'tv', 'post', 'comment', 'fox', 'radio', 'conference', 'television', 'presidential', 'republican', 'statement', 'cnn']\n",
      "[1754 1062  877  868  809  768  687  671  498  495  381  344  339  336\n",
      "  300  282  252  247  245  237]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/STA561/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "desired_feature = \"context\"\n",
    "subjects_encoder = pre_processor.get_bag_of_words_encoder_for_feature(\n",
    "    feature_name=desired_feature,\n",
    "    clean_strings=True,\n",
    "    remove_stop_words=True,\n",
    "    lematize=True\n",
    ")\n",
    "\n",
    "unique_words = subjects_encoder.encoding_mappings.keys()\n",
    "\n",
    "features,counts = subjects_encoder.get_most_common(20)\n",
    "print(features)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['energy', 'history', 'job', 'accomplishments']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "test_str = 'energy,history,job-accomplishments'\n",
    "tokens = tokenizer.tokenize(test_str)\n",
    "print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STA561",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ff78712972ad1d9723777b77156440718eac6d60d034e137fcbf0563820b527"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
